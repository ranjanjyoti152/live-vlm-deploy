# ==============================================================================
# Live VLM WebUI - Docker Compose
# ==============================================================================
# Two deployment modes available:
#
# 1. External Ollama (Default) - Use existing Ollama on host
#    Usage: docker compose up -d
#    Requires: Ollama running on host at http://localhost:11434
#
# 2. Ollama in Docker - Run Ollama as a container
#    Usage: docker compose --profile ollama up -d
#    Automatically starts Ollama container with GPU support
#
# ==============================================================================
# Quick Start Commands:
# ==============================================================================
# External Ollama (default):
#   docker compose up -d
#
# With Ollama in Docker:
#   docker compose --profile ollama up -d
#   docker exec ollama ollama pull llama3.2-vision:11b
#
# Access:
#   - Live VLM WebUI: https://localhost:8090
#   - Ollama API: http://localhost:11434/v1
#
# Stop:
#   docker compose down
# ==============================================================================

services:
  # ============================================================================
  # Ollama Service (Optional - enabled with --profile ollama)
  # ============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    profiles:
      - ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ============================================================================
  # Live VLM WebUI - Works with External or Containerized Ollama
  # ============================================================================
  live-vlm-webui:
    image: ghcr.io/nvidia-ai-iot/live-vlm-webui:latest
    container_name: live-vlm-webui
    # Using host network mode to:
    # 1. Access Ollama on localhost:11434 (external or containerized)
    # 2. Enable WebRTC for webcam streaming
    network_mode: host
    environment:
      - PYTHONUNBUFFERED=1
      # Optional: Pre-configure API endpoint (can be changed in WebUI)
      - API_BASE=http://localhost:11434/v1
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, utility]

volumes:
  ollama-data:
    driver: local

# ==============================================================================
# Deployment Modes:
# ==============================================================================
# Mode 1: External Ollama (Default)
#   - Use your existing Ollama installation on the host
#   - Command: docker compose up -d
#   - Requires: Ollama running at http://localhost:11434
#   - Lighter on resources (no extra container)
#
# Mode 2: Ollama in Docker
#   - Run Ollama as a container alongside the WebUI
#   - Command: docker compose --profile ollama up -d
#   - Self-contained, no need for host Ollama installation
#   - Models stored in Docker volume (persistent)
#   - Pull models: docker exec ollama ollama pull llama3.2-vision:11b
#
# ==============================================================================
# Notes:
# ==============================================================================
# 1. Both modes connect to Ollama at http://localhost:11434
#    - External mode: Ollama runs on host
#    - Docker mode: Ollama runs in container, exposed via port mapping
#
# 2. Make sure to install vision models:
#    External: ollama pull llama3.2-vision:11b
#    Docker:   docker exec ollama ollama pull llama3.2-vision:11b
#
# 3. GPU access is configured for NVIDIA GPUs. If you don't have a GPU,
#    remove the 'deploy' sections.
#
# 4. To switch modes, stop and restart with the appropriate command:
#    docker compose down
#    docker compose [--profile ollama] up -d
# ==============================================================================
