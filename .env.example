# ==============================================================================
# Live VLM WebUI - Environment Configuration
# ==============================================================================
# Copy this file to .env and customize as needed
# Usage: cp .env.example .env
# ==============================================================================

# Ollama API Configuration
# -------------------------
# Ollama API endpoint (if running externally)
API_BASE=http://localhost:11434/v1

# Optional: Default model to use (can be changed in WebUI)
# DEFAULT_MODEL=llama3.2-vision:11b

# WebUI Configuration
# -------------------
# Port for the WebUI (default: 8090)
# WEBUI_PORT=8090

# Host for the WebUI (default: 0.0.0.0)
# WEBUI_HOST=0.0.0.0

# SSL Configuration (auto-generated certificates are used by default)
# SSL_CERT=cert.pem
# SSL_KEY=key.pem

# ==============================================================================
# Notes:
# ==============================================================================
# 1. Ensure Ollama is running: ollama serve
# 2. Install a vision model: ollama pull llama3.2-vision:11b
# 3. Start the stack: docker compose up -d
# 4. Access WebUI: https://localhost:8090
# ==============================================================================
